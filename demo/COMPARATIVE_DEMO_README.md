# ğŸ† LLM-as-a-Judge Comparative Demo

This demo showcases **Baseline vs ACE** code review with an **LLM judge** that scores each answer from 0-100%.

## ğŸ¯ How It Works

1. **Both models analyze the same code** (sequentially)
   - ğŸ”µ **Baseline**: Junior engineer with no strategies (empty playbook)
   - ğŸŸ¢ **ACE**: Senior expert with 21 pre-trained strategies

2. **LLM Judge compares the answers** (Claude Haiku for speed/cost)
   - Evaluates on 4 criteria:
     - **Correctness (40%)**: Does it identify actual bugs from ground truth?
     - **Completeness (30%)**: Does it catch ALL bugs, including edge cases?
     - **Explanation Quality (20%)**: Are explanations clear and insightful?
     - **Solution Quality (10%)**: Are proposed fixes correct and practical?

3. **Round-by-round results displayed**
   - Side-by-side comparison
   - Scores (0-100%)
   - Strengths & weaknesses analysis
   - Token counts for each model
   - Winner per round

4. **Final summary** shows overall stats

## ğŸš€ Quick Start

### 1. Pre-train ACE (One-time)

```bash
python demo/pretrain_playbook.py
```

This trains ACE on all 10 samples and saves the playbook to `demo/pretrained_playbook.json`.

### 2. Run the Comparative Demo

```bash
python demo/api_server_comparative.py
```

### 3. Open Browser

Navigate to: **http://localhost:8001**

Click **"Start Competition!"** and watch the race!

## ğŸ“Š What You'll See

### Round-by-Round Display

Each round shows:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Round 1                                       âœ… Judged      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  ğŸ”µ Baseline (Junior)        â”‚  ğŸŸ¢ ACE (Senior Expert)      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚  Time: 45.2s                 â”‚  Time: 38.7s                 â”‚
â”‚  Tokens: 8,542               â”‚  Tokens: 6,234               â”‚
â”‚                               â”‚                               â”‚
â”‚  Judge Score: 62/100         â”‚  Judge Score: 94/100         â”‚
â”‚                               â”‚                               â”‚
â”‚  âœ… Strengths:               â”‚  âœ… Strengths:               â”‚
â”‚  - Identified main issue     â”‚  - Comprehensive analysis    â”‚
â”‚                               â”‚  - All edge cases caught     â”‚
â”‚  âŒ Weaknesses:              â”‚  - Multiple fix options      â”‚
â”‚  - Missed edge cases         â”‚                               â”‚
â”‚  - Incomplete explanation    â”‚  âŒ Weaknesses:              â”‚
â”‚                               â”‚  - Slightly verbose          â”‚
â”‚                                                               â”‚
â”‚  âš–ï¸ Judge Verdict                                           â”‚
â”‚  Winner: ğŸŸ¢ ACE                                              â”‚
â”‚  Reasoning: ACE provided comprehensive analysis with...      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Overall Statistics (Top of page)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ”µ Baseline (Junior) â”‚  âš–ï¸ LLM Judge       â”‚ ğŸŸ¢ ACE (Senior)     â”‚
â”‚ 65.2%                â”‚  2,145 tokens        â”‚ 89.7%                â”‚
â”‚ Avg Score            â”‚  Judging Tokens      â”‚ Avg Score            â”‚
â”‚ 29,843 tokens        â”‚                      â”‚ 21,456 tokens        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Final Winner Banner

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                        â•‘
â•‘           ğŸŸ¢ ACE Wins!                â•‘
â•‘                                        â•‘
â•‘  Average Scores: Baseline 65.2% vs    â•‘
â•‘                  ACE 89.7%             â•‘
â•‘                                        â•‘
â•‘  Total Tokens: Baseline 29,843 |      â•‘
â•‘                ACE 21,456              â•‘
â•‘                                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

## ğŸ¨ Features

âœ… **LLM-as-a-Judge scoring** (0-100% per answer)
âœ… **Round-by-round results** with detailed breakdowns
âœ… **Strengths & weaknesses** for each answer
âœ… **Token tracking** for all models (baseline, ACE, judge)
âœ… **Beautiful comparative UI** with side-by-side display
âœ… **Winner determination** per round
âœ… **Overall statistics** and final summary
âœ… **Expandable code views** for full answers and ground truth

## ğŸ”§ Configuration

### Judge Model

Default: `claude-3-5-haiku-20241022` (fast and cost-effective)

Change in `demo/api_server_comparative.py`:

```python
judge = LLMJudge(model="claude-3-5-haiku-20241022")
```

### Number of Samples

Default: 4 samples

Change in `demo/api_server_comparative.py`:

```python
samples_raw = get_race_samples(count=4)  # Change to 2, 6, 8, or 10
```

### Judge Criteria Weights

Modify in `demo/llm_judge.py`:

```python
1. **Correctness (40%)**: ...
2. **Completeness (30%)**: ...
3. **Explanation Quality (20%)**: ...
4. **Solution Quality (10%)**: ...
```

## ğŸ“ˆ Expected Results

With the current **expert-level bug challenges**:

### Baseline (Junior Engineer)
- **Avg Score**: 50-70%
- Catches obvious bugs
- Misses edge cases and subtle issues
- Less comprehensive explanations
- Uses more tokens (exploratory reasoning)

### ACE (Senior Expert)
- **Avg Score**: 80-95%
- Catches all bugs including edge cases
- Recognizes patterns from playbook
- Comprehensive explanations
- Uses fewer tokens (focused expertise)

### Token Efficiency
- **ACE typically uses 20-30% fewer tokens** due to learned strategies
- Judge uses ~500-1000 tokens per comparison

## ğŸ†š vs Original Demo

### Original Demo (`api_server.py`, port 8000)
- Side-by-side race visualization
- Real-time progress tracking
- Simple accuracy scores from keyword matching

### Comparative Demo (`api_server_comparative.py`, port 8001)
- **Sequential execution** (one after the other)
- **LLM judge** provides nuanced 0-100% scoring
- **Detailed analysis** with strengths/weaknesses
- **Round-by-round results**
- More comprehensive but slower

## ğŸ“ Use Cases

### 1. Demonstrating ACE Value
Show stakeholders the **quality difference** between baseline and ACE with objective LLM judging.

### 2. Evaluating Playbook Quality
See how well ACE's learned strategies translate to better bug detection.

### 3. Comparing Different Prompts
Test different system prompts by modifying the context strings.

### 4. Benchmarking Models
Compare different base models (GPT-4, Claude, etc.) with/without ACE.

## ğŸ’¡ Tips

1. **Run both demos** to see different perspectives:
   - Original: Fast, parallel, visual race
   - Comparative: Detailed, judged, analytical

2. **Watch token counts** - ACE's efficiency advantage is clear

3. **Read the judge reasoning** - provides insights into what makes a good analysis

4. **Check strengths/weaknesses** - shows where each model excels or fails

## ğŸ› Troubleshooting

### "Playbook not loaded"
Run pre-training first: `python demo/pretrain_playbook.py`

### Port already in use
Original demo uses 8000, comparative uses 8001. Stop other servers or change ports.

### Judge errors
Check API keys: `ANTHROPIC_API_KEY` must be set for Claude Haiku judge.

### Slow performance
Normal! Each round involves:
- 2 deep reasoning LLM calls (baseline + ACE)
- 1 judge LLM call
- Total: 3 LLM calls per round

## ğŸ“ Example Session

```bash
# 1. Pre-train (one time)
$ python demo/pretrain_playbook.py
ğŸ§  ACE Pre-Training Script
ğŸ“š Total samples: 10
âœ… Training complete! Learned 21 strategies

# 2. Run comparative demo
$ python demo/api_server_comparative.py
ğŸš€ Starting Comparative Bug Hunter Demo Server...
ğŸ“Š Race: 4 bug samples with LLM-as-a-judge
ğŸ”µ Baseline: Junior Engineer (no strategies)
ğŸŸ¢ ACE: Senior Expert (with playbook strategies)
âš–ï¸  Judge: Claude Haiku comparing answers
âœ… Playbook loaded with 21 strategies

# 3. Open http://localhost:8001 and watch!
```

## ğŸ‰ Expected Outcome

ACE should **consistently outperform** baseline with:
- **Higher scores** (15-25% better on average)
- **Fewer tokens** (20-30% more efficient)
- **Better completeness** (catches all bugs)
- **Superior explanations** (leverages learned strategies)

This demonstrates the **real value of ACE** - not just faster, but **significantly better quality** with fewer resources! ğŸš€

